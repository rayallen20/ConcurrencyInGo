# Concurrency In Go

## 第4章 GO语言并发编程范式

### 队列排队

在之前的章节我们列举了pipeline的各种优点.但有时候,你需要在pipeline尚未就绪的前提下就开始接受请求,这一点是很有用的.这个过程被称为队列.

这也就意味着一旦pipeline中的某个阶段完成了一些工作,它就要把结果存储在内存中的某个临时区域,以便后续的其他阶段可以找到这个结果,且这个完成工作的阶段不需要保持对这个结果的引用.实际上我们之前在[第3章 GO语言并发组件 channel章节](https://github.com/rayallen20/ConcurrencyInGo/blob/master/note/%E7%AC%AC3%E7%AB%A0%20GO%E8%AF%AD%E8%A8%80%E5%B9%B6%E5%8F%91%E7%BB%84%E4%BB%B6/3.%20channel.md#%E7%BC%93%E5%86%B2%E7%AE%A1%E9%81%93)中我们已经讨论了带有缓冲区的channel,其实它就是一种队列,只是之前我们没有真正的使用它,这是有原因的.

尽管将队列引入你的系统是非常有用的,但是它通常是优化系统的最后手段.过早地引入队列会导致同步问题(例如死锁和活锁)的隐藏.此外,随着程序朝着正确性收敛,你可能会发现你需要更多或更少的队列.

那么队列的好处在哪儿呢?让我们通过一个在系统调优时常见的错误来回答这个问题:引入队列试图解决性能问题.**队列几乎永远不能减少你程序的总运行时长,它只能让你程序的行为变得不同**.

让我们通过一个例子来理解这句话.

```go
done := make(chan interface{})
defer close(done)

zeros := take(done, 3, repeat(done, 0))
short := sleep(done, 1 * time.Second, zeros)
long := sleep(done, 4 * time.Second, short)
pipeline := long
```

这个pipeline共4个阶段:

1. `repeat()`阶段:生成一个包含无限个0的流的阶段
2. `take()`阶段:从`repeat()`阶段中取出3个0之后,取消`repeat()`阶段的阶段
3. `short`阶段:一个短耗时阶段,休眠1s
4. `long`阶段:一个长耗时阶段,休眠4s

此处我们假定`repeat()`阶段和`short()`阶段是瞬时完成的,我们只关注2个`sleep()`阶段是如何对pipeline的整体运行时间带来影响的.以下表格显示了时间`t`、迭代`i`(即pipeline中正在处理第几个0)以及`short`阶段和`long`阶段分别还剩下多少秒才会继续处理下一个值

|Time(t)|i|`long`阶段|`short`阶段|备注|
|:-:|:-:|:-:|:-:|:-:|
|0|0|尚未开始|1s|初态:整个pipeline尚未开始运行.但`short`已经执行了1次(TODO:我不知道为什么这里初态时就执行了1次).|
|1|0|4s|1s|`long`阶段从`short`阶段中拿到了第1个0,可以开始运行了,即开始等待4s;`short`阶段在上一秒产生的运行结果被`long`取走,因此也可以继续开始第2次迭代|
|2|0|3s|(blocked)|`long`阶段还需等待3s;`short`阶段此时由于没有人来取第2次迭代的结果,因此被阻塞|
|3|0|2s|(blocked)|`long`阶段还需等待2s;`short`阶段此时由于没有人来取第2次迭代的结果,因此被阻塞|
|4|0|1s|(blocked)|`long`阶段还需等待1s;`short`阶段此时由于没有人来取第2次迭代的结果,因此被阻塞|
|5|1|4s|1s|`long`阶段完成了第1次迭代,这样就宣布整个pipeline完成了第1次迭代,同时在这1s内,`long`阶段再次从`short`阶段中取出了第2个0,开始了该阶段的第2次迭代,即再次等待4s;`short`阶段由于第2次迭代的结果被取走,因此取消阻塞,进入第3次迭代|
|6|1|3s|(blocked)|`long`阶段还需等待3s;`short`阶段此时由于没有人来取第3次(即最后一次)迭代的结果,因此被阻塞|
|7|1|2s|(blocked)|`long`阶段还需等待2s;`short`阶段此时由于没有人来取第3次(即最后一次)迭代的结果,因此被阻塞|
|8|1|1s|(blocked)|`long`阶段还需等待1s;`short`阶段此时由于没有人来取第3次(即最后一次)迭代的结果,因此被阻塞|
|9|2|4s|(closed)|`long`阶段完成了第2次迭代,这样就宣布整个pipeline完成了第2次迭代,同时在这1s内,`long`阶段再次从`short`阶段中取出了第3个0,开始了该阶段的第3次迭代,即再次等待4s;`short`阶段由于最后一次迭代的结果被取走,因此完成任务,关闭channel|
|10|2|3s|(closed)|`long`阶段还需等待3s;`short`阶段的返回值已关闭|
|11|2|2s|(closed)|`long`阶段还需等待2s;`short`阶段的返回值已关闭|
|12|2|1s|(closed)|`long`阶段还需等待1s;`short`阶段的返回值已关闭|
|13|3|(closed)|(closed)|`long`阶段也完成了自己最后一次迭代,因此关闭返回值channel,整个pipeline结束|


如果我们在pipeline中加入缓冲区,会发生什么?我们在`short`阶段和`long`阶段之间引入一个长度为2的缓冲区:

```go
done := make(chan interface{})
defer close(done)

zeros := take(done, 3, repeat(done, 0))
short := sleep(done, 1 * time.Second, zeros)
// 给short阶段增加一个缓冲区 长度为2
buffer := buffer(done, 2, short)
long := sleep(done, 4 * time.Second, buffer)
pipeline := long
```

|Time(t)|i|`long`阶段|缓冲区占用情况|`short`阶段|备注|
|:-:|:-:|:-:|:-:|:-:|:-:|
|0|0|尚未开始|0/2|1s|初态:整个pipeline尚未开始运行.但`short`已经执行了1次(TODO:我不知道为什么这里初态时就执行了1次).|
|1|0|4s|0/2|1s|`long`阶段从`short`阶段中直接拿到第1个0开始工作,即等待4s;`short`阶段开始第2次迭代|
|2|0|3s|1/2|1s|`long`阶段还需等待3s;`short`阶段将第2次迭代的结果存入缓冲区,开始第3次迭代|
|3|0|2s|2/2|(closed)|`long`阶段还需等待2s;`short`阶段第3次(即最后一次)迭代结束,将第3次迭代的结果存入缓冲区.`short`阶段完成任务,关闭返回的channel|
|4|0|1s|2/2|(closed)|`long`阶段还需等待1s|
|5|1|4s|1/2|(closed)|`long`阶段完成了第1次迭代,即整个pipeline完成了第1次迭代.`long`阶段从缓冲区内取出第2个0,开始第2次迭代,即:等待4s|
|6|1|3s|1/2|(closed)|`long`阶段还需等待3s|
|7|1|2s|1/2|(closed)|`long`阶段还需等待2s|
|8|1|1s|1/2|(closed)|`long`阶段还需等待1s|
|9|2|4s|0/2|(closed)|`long`阶段完成了第2次迭代,即整个pipeline完成了第2次迭代.`long`阶段从缓冲区内取出第3个0,开始第3次迭代,即:等待4s|
|10|2|3s|0/2|(closed)|`long`阶段还需等待3s|
|11|2|2s|0/2|(closed)|`long`阶段还需等待2s|
|12|2|1s|0/2|(closed)|`long`阶段还需等待1s|
|13|2|(closed)|0/2|(closed)|`long`阶段完成第3次(即最后一次)迭代,关闭返回的channel.这意味着整个pipeline完成了最后一次迭代|

可以看到,整个pipeline依然需要13s才能完成迭代,但其中的`short`阶段的耗时从之前的9s降低到了3s.也就是说我们通过加入缓冲区,使得`short`阶段的运行时间缩短了2/3.但是整个pipeline还是需要13s才能执行完毕,那么缩短`short`阶段的运行时间这件事,意义何在呢?

来看下面这个操作:

```go
p := processRequest(done, acceptConnection(done, httpHandler))
```

这个pipeline直到从`done`channel中收到取消信号,整个pipeline才会退出,而`acceptConnection`阶段会在pipeline退出前一直接受连接.这种场景下,你不希望由于`processRequest`阶段阻塞`acceptConnection`阶段而导致整个程序请求超时.你希望`acceptConnection`阶段尽可能的不被阻塞.否则用户可能会看到他们的请求完全被拒绝.

因此,引入队列并不能减少某个阶段的运行时间,而是减少这个阶段处于阻塞状态的时间.这可以让这个阶段继续完成其他工作.在这个例子中,用户可能会在他们的请求中感受到延迟,但不会被拒绝服务.

可以看到,队列的真正用途是将操作流程分离,以便一个阶段的运行时间不会影响另一个阶段的运行时间.以这种方式解耦各个阶段,会改变整个系统的运行时行为.而这种改变对于整个系统而言可能是好的也可能是坏的,这取决于具体情况.

然后我们还是回到关于队列的讨论.队列应该放在哪里?缓冲区的大小应该是多少?这些问题的答案取决于你的pipeline的性质.

首先分析队列能够提高系统整理性能的情况.适用的情况包括:

- 若在一个阶段中,批处理请求可以节省时间
- 若在一个阶段中产生的延迟会在系统中产生一个反馈循环

先来分析第1种情况.适用于第1种情况的例子:将输入缓冲到内存而非磁盘中.实际上`bufio`包就是这么做的.本质上是**一个阶段的输出速度大于其发送目标的处理速度**.

例:带有缓冲区和不带缓冲区的写入比较

```go
package main

import (
	"bufio"
	"io"
	"io/ioutil"
	"log"
	"os"
	"testing"
)

// BenchmarkUnbufferedWrite 直接向文件写入
func BenchmarkUnbufferedWrite(b *testing.B) {
	performWrite(b, tmpFileOrFatal())
}

// BenchmarkBufferedWrite 使用带缓冲的writer向文件写入
func BenchmarkBufferedWrite(b *testing.B) {
	// 创建一个带缓冲的writer
	bufferedFile := bufio.NewWriter(tmpFileOrFatal())
	performWrite(b, bufferedFile)
}

// tmpFileOrFatal 创建临时文件
func tmpFileOrFatal() *os.File {
	file, err := ioutil.TempFile("", "tmp")
	if err != nil {
		log.Fatalf("error: %v\n", err)
	}
	return file
}

// performWrite 向给定的writer中写入
func performWrite(b *testing.B, writer io.Writer) {
	done := make(chan interface{})
	defer close(done)

	b.ResetTimer()
	for bt := range take(done, repeat(done, byte(0)), b.N) {
		writer.Write([]byte{bt.(byte)})
	}
}

func repeat(done <-chan interface{}, values ...interface{}) <-chan interface{} {
	valueStream := make(chan interface{})

	go func() {
		defer close(valueStream)
		for {
			for _, value := range values {
				select {
				case <-done:
					return
				case valueStream <- value:
				}
			}
		}
	}()

	return valueStream
}

func take(done <-chan interface{}, valueStream <-chan interface{}, num int) <-chan interface{} {
	takeStream := make(chan interface{})

	go func() {
		defer close(takeStream)

		for i := 0; i < num; i++ {
			select {
			case <-done:
				return
			case takeStream <- <-valueStream:
			}
		}
	}()

	return takeStream
}
```

测试结果如下:

```
go test -bench=. compareBufferAndNoBuffer_test.go 
goos: darwin
goarch: amd64
cpu: Intel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz
BenchmarkUnbufferedWrite-8        215179              5173 ns/op
BenchmarkBufferedWrite-8         1443162               809.9 ns/op
PASS
ok      command-line-arguments  5.263s
```

可以看到,有缓冲的写入比无缓冲的更快.这是因为在`bufio.Writer`中,写入在内部排队到缓冲区中,直到积累了足够长的数据块,这个块才被写出.这个过程通常称为分块.

分块的速度更快,是因为`bytes.Buffer`必须增加分配给它的内存空间以便容纳它要存储的字节.由于各种原因(内存拷贝等),内存扩张操作的代价是高昂的;因此内存扩张的次数越少,整个系统的性能就越高.这是一个通过使用队列来提高整体系统性能的例子.

这只是一个简单的内存分块的示例,但在实际应用场景下,你可能会经常遇到分块操作.通常**每当执行的操作需要一定的开销时,分快操作都可以提高系统的性能**.一些例子包括:打开数据库事务、计算消息校验和和分配连续内存空间.

除分块外,如果你的程序算法支持后向查找或排序优化,队列也可以起到提升性能的作用.

第二种情况,当一个阶段的延迟导致了更多的输入进入pipeline时,这种情况不太容易被发现,但这种情况更为重要,因为它可能导致上游系统的系统性崩溃.

这个概念通常被称为:负反馈循环、下降螺旋、死亡螺旋.这是因为pipeline和它的上游系统之间存在一个循环关系.pipeline中的上游阶段或pipeline的上游系统提交心情求的速率在某种程度上与pipeline的效率存在某种关联.

如果pipeline的效率低于某个临界阈值,则pipeline上游的系统会开始增加它对pipeline的输入,导致pipeline更加低效,从而开始出现死亡螺旋.如果没有某种故障保护机制,上游系统使用的这个pipeline将无法恢复.

通过在pipeline的入口处引入队列,你可以打破这个反馈循环,但代价是请求会有延迟.从pipeline调用者的视角来看,请求似乎是在处理,只是花费的时间较长.只要调用者不超时,pipeline就能保持稳定.如果调用者设置了某种超时重试机制,则你要确保你的pipeline在出列时支持某种就绪检查.如果没有这种就绪检查机制,你可能会因为无意间处理无效请求而导致创建另一个反馈循环,从而导致pipeline效率的降低.

##### 你有没有见过死亡螺旋?

如果你曾经尝试在某个热门新系统上线时访问(例如新的游戏服务器、产品发布的网站等),但无论开发人员如何努力,该网站仍然不稳定,那么恭喜你!你可能目睹了一个负反馈循环.开发团队总是尝试不同的方法,直到有人意识到他们需要一个队列,然后匆忙实施.然后客户开始抱怨队列等待时间!

****

我的理解:说白了就是越卡越刷新,越刷新越卡

****

从以上的例子中,我们可以看到一个模式,队列应该在以下场景使用:

- 在pipeline的入口处
- pipeline中的某个阶段进行批处理会更加高效

你可能试图在其他地方加入队列(例如在一个计算密集型的阶段后边加一个队列),要避免这种诱惑!正如我们所了解的,只有少数场景中,队列能够减少pipeline的运行时间,而且以遍布队列的方式让程序运行起来,会产生灾难性的后果.

这一点起初可能不太直观.为了理解其中的原因,我们需要讨论pipeline的吞吐量.别担心,这并不困难,而且它还会帮助我们回答一个问题:如何确定队列的大小.

在排队论中,有一个定律:在有足够采样的前提条件下,可以预测pipeline的吞吐量.这被称为"利特尔法则".你只需要了解一些基本概念就能理解并应用它.

我们以代数的方式定义"利特尔法则",它通常表示为:`L = λW`,其中:

- `L`:系统中平均负载数
- `λ`:负载的平均速率
- `W`:负载在系统中花费的平均时间

这个等式仅适用于所谓的稳定系统.在一个pipeline中,所谓的稳定指的是:数据进入pipeline的速率(入口速率)等于数据离开pipeline的速率(出口速率).若入口速率大于出口速率,则你的系统就是不稳定的,且整个pipeline进入了死亡螺旋模式.若入口速率小于出口速率,则你的资源没有被充分的利用.虽然这并不是最糟糕的情况,但如果出现大规模低资源利用率的情况(例如集群或数据中心),你可能会关心这个问题.

假设我们的pipeline是稳定的.假设我们想要将W(负载在系统中花费的平均时间)减少n倍,那么我们只有1个选择:将L减少n倍(即:将系统中平均负载数减少n倍).由此可以得到:`L/n = λ * W/n`.如果我们想要增加出口速率,则只能减少L(系统中的平均负载数).同时要注意,我们在pipeline的阶段中添加了队列,也就是说实际上我们增加了L(系统中平均负载数).L的增加要么导致负载平均速率的增加(`nL = nλ * W`),要么导致负载在系统中花费平均时间的增加(`nL = λ * nW`).通过利特尔法则,我们可以证明一件事:**队列无法减少数据在系统中花费的时间**.

同时要注意,我们将pipeline视为一个整体,因此对W(负载在系统中花费的平均时间)减少n倍这件事会在pipeline的所有阶段中分布,在我们的例子中,利特尔法则的表达式应该是:

`L=λ∑{i}W{i}`

这意味着你的pipeline的速度将取决于pipeline中最慢的阶段.因此不要瞎优化.

利特尔法则确实很有用.这个简单的公式为我们分析pipeline提供了各种方法.让我们利用它来提出一些有趣的问题.在我们的分析中,假设我们的pipeline中有3个阶段.

让我们尝试确定我们的pipeline每秒可以处理多少个请求.假定我们对pipeline的采样显示每个请求(r)需要大约1s才能通过pipeline.让我们把这些数字代入到公式中.

```
3r = λr/s * 1s(L=3,W=1,求λ的值)

3r/s = λr/s

λr/s = 3r/s
```

由此可得,`λ=3`.此处由于pipeline中的每个阶段都在处理请求,所以`L=3`.由于每个请求需要1s才能通过pipeline,因此`W=1`.代入利特尔法则可知`λ=3`.即:pipeline每秒可以处理3个请求.

关于确定我们的队列需要多大才能处理给定数量的请求,Little定律能帮助我们回答这个问题吗?

那么假设采样表明每个请求通过pipeline需要1ms,当pipeline需面对的入口速率为100,000r/s时,求pipeline的入口处队列长度应该为多少?

```
Lr = 100,000r/s * 0.001s

Lr = 100r

由于pipeline中有3个阶段,因此所求出的L应该再减去3.即:

L = L - 3
```

由于pipeline中有3个阶段,这意味着本来pipeline最多就能同时处理3个请求.所以在等式的左侧加3r.`λ`为速率,是题设中给出的100,000r/s,`W`为请求在pipeline中所花费的时间,是题设中给出的1ms(即0.0001s),求L.记住:**当你增加队列的大小时,实际上完成工作的时间反而会变长**.实际上是在用系统利用率在换延迟.

但利特尔法则也是存在缺陷的.因为它无法提供对故障的处理.如果因为某些原因你的pipeline触发了panic,你会丢失pipeline中所有的请求.如果不可能重建请求或重建请求很困难,这可能就是一个需要防范的问题.为了避免这种问题,你可以使用一个0长度的队列,或者将请求先存储到一个持久队列中,这个队列持久存在于某个位置,以备将来需要时进行读取.

队列在你的系统中可能很有用,但由于它的复杂性,它通常是我建议实现的最后优化手段之一.